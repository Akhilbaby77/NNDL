{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "# XOR outputs (truth table)\n",
        "y = np.array([0, 1, 1, 0])\n"
      ],
      "metadata": {
        "id": "Xy4jvi1pAT2T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.1, epochs=10):\n",
        "        self.weights = np.zeros(input_size + 1)  # Including bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def activation(self, x):\n",
        "        return 1 if x >= 0 else 0  # Step function (MCP activation)\n",
        "\n",
        "    def predict(self, x):\n",
        "        weighted_sum = np.dot(x, self.weights[1:]) + self.weights[0]  # w.X + bias\n",
        "        return self.activation(weighted_sum)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for i in range(len(X)):\n",
        "                prediction = self.predict(X[i])\n",
        "                error = y[i] - prediction\n",
        "                self.weights[1:] += self.learning_rate * error * X[i]\n",
        "                self.weights[0] += self.learning_rate * error  # Bias update\n",
        "                total_error += abs(error)\n",
        "            print(f'Epoch {epoch+1}/{self.epochs}, Total Error: {total_error}')\n",
        "            if total_error == 0:\n",
        "                break\n",
        "        print(f'Final Weights: {self.weights}')\n",
        "\n",
        "# Train perceptron on XOR data\n",
        "p = Perceptron(input_size=2, learning_rate=0.1, epochs=10)\n",
        "p.train(X, y)\n",
        "\n",
        "# Test perceptron\n",
        "for x in X:\n",
        "    print(f'Input: {x}, Predicted Output: {p.predict(x)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "shO-yeL3AZHD",
        "outputId": "fc94ed9a-2d22-426a-810e-1400e017d121"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Total Error: 3\n",
            "Epoch 2/10, Total Error: 3\n",
            "Epoch 3/10, Total Error: 4\n",
            "Epoch 4/10, Total Error: 4\n",
            "Epoch 5/10, Total Error: 4\n",
            "Epoch 6/10, Total Error: 4\n",
            "Epoch 7/10, Total Error: 4\n",
            "Epoch 8/10, Total Error: 4\n",
            "Epoch 9/10, Total Error: 4\n",
            "Epoch 10/10, Total Error: 4\n",
            "Final Weights: [ 0.  -0.1  0. ]\n",
            "Input: [0 0], Predicted Output: 1\n",
            "Input: [0 1], Predicted Output: 1\n",
            "Input: [1 0], Predicted Output: 0\n",
            "Input: [1 1], Predicted Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the perceptron fails to classify the XOR gate correctly. This is because the XOR gate is not linearly separable, and a single-layer perceptron can only solve linearly separable problems."
      ],
      "metadata": {
        "id": "LH9EnKXQAqgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Create MLP model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(2,)))  # Define the input layer with shape\n",
        "model.add(Dense(2, activation='relu'))  # Hidden layer with 2 neurons\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer with 1 neuron\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (increase epochs to improve performance)\n",
        "history = model.fit(X, y, epochs=500, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X, y)\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Predict the outputs\n",
        "predictions = model.predict(X)\n",
        "predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
        "for i in range(len(X)):\n",
        "    print(f'Input: {X[i]}, Predicted Output: {predictions[i]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Pv6xHhS_Arqd",
        "outputId": "bacb496a-c09a-4a11-bf78-0079ffc46bde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.5757\n",
            "Model Accuracy: 100.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Input: [0 0], Predicted Output: 0\n",
            "Input: [0 1], Predicted Output: 1\n",
            "Input: [1 0], Predicted Output: 1\n",
            "Input: [1 1], Predicted Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Single Layer Perceptron: The single-layer perceptron fails to solve the XOR gate problem because it cannot draw a linear boundary to classify non-linearly separable data.\n",
        "\n",
        "Multi-Layer Perceptron (MLP): The MLP can successfully classify the XOR gate's outputs because it introduces a hidden layer with non-linear activation functions (ReLU). This allows it to learn non-linear decision boundaries.\n",
        "\n",
        "\n",
        "A Single Layer Perceptron is insufficient for solving the XOR problem due to the linear separability constraint.\n",
        "A Multi-Layer Perceptron successfully classifies XOR by introducing non-linearities via hidden layers."
      ],
      "metadata": {
        "id": "FC9zvBBeBnON"
      }
    }
  ]
}